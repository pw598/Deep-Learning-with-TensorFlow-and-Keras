{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07f4437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 5000).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 5000).\n",
      "Model: \"skipgram_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 300)               1500000   \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 300)               1500000   \n",
      "_________________________________________________________________\n",
      "dot (Dot)                    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  2         \n",
      "=================================================================\n",
      "Total params: 1,500,002\n",
      "Trainable params: 1,500,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[-0.01100422 -0.01327725 -0.02641682 ...  0.00251868  0.0283801\n",
      "  -0.00623294]\n",
      " [-0.00198391 -0.01823014  0.02847448 ...  0.033565   -0.02309088\n",
      "  -0.02090983]\n",
      " [-0.00064245  0.02141165  0.00219676 ... -0.02306221 -0.00551796\n",
      "   0.02217396]\n",
      " ...\n",
      " [-0.03318698  0.01423855  0.03354494 ... -0.02394232  0.01036642\n",
      "   0.01606221]\n",
      " [-0.0076992   0.01703976  0.0136196  ... -0.01000518  0.0012406\n",
      "  -0.00201255]\n",
      " [ 0.02670559  0.00153694  0.0041175  ... -0.00883408  0.0026454\n",
      "   0.00814663]] (5000, 300)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SkipgramModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_sz, embed_sz, **kwargs):\n",
    "        super(SkipgramModel, self).__init__(**kwargs)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_sz,\n",
    "            output_dim=embed_sz,\n",
    "            embeddings_initializer=\"glorot_uniform\",\n",
    "            input_length=1\n",
    "        )\n",
    "        self.word_model = tf.keras.Sequential([\n",
    "            embedding,\n",
    "            tf.keras.layers.Flatten()\n",
    "        ])\n",
    "        self.context_model = tf.keras.Sequential([\n",
    "            embedding,\n",
    "            tf.keras.layers.Flatten()\n",
    "        ])\n",
    "        self.merge = tf.keras.layers.Dot(axes=1)\n",
    "        self.dense = tf.keras.layers.Dense(1,\n",
    "                kernel_initializer=\"glorot_uniform\",\n",
    "                activation=\"sigmoid\"\n",
    "        )\n",
    "\n",
    "    def call(self, input):\n",
    "        word, context = input\n",
    "        word_emb = self.word_model(word)\n",
    "        context_emb = self.context_model(context)\n",
    "        x = self.merge([word_emb, context_emb])\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "EMBED_SIZE = 300\n",
    "\n",
    "model = SkipgramModel(VOCAB_SIZE, EMBED_SIZE)\n",
    "model.build(input_shape=[(None, VOCAB_SIZE), (None, VOCAB_SIZE)])\n",
    "model.compile(optimizer=tf.optimizers.Adam(),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# train the model here\n",
    "\n",
    "# retrieve embeddings from trained model\n",
    "word_model = model.layers[0]\n",
    "word_emb_layer = word_model.layers[0]\n",
    "emb_weights = None\n",
    "for weight in word_emb_layer.weights:\n",
    "    if weight.name == \"embedding/embeddings:0\":\n",
    "        emb_weights = weight.numpy()\n",
    "print(emb_weights, emb_weights.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc65f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
