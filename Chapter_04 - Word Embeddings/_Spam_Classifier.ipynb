{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd88b25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--mode {scratch,vectorizer,finetuning}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\patwh\\AppData\\Roaming\\jupyter\\runtime\\kernel-506cc747-5415-46a9-ad90-78bf3ff6b1ee.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, \n",
    "        extract=True, cache_dir=\".\")\n",
    "    labels, texts = [], []\n",
    "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == \"spam\" else 0)\n",
    "            texts.append(text)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def build_embedding_matrix(sequences, word2idx, embedding_dim, \n",
    "        embedding_file):\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)\n",
    "    else:\n",
    "        vocab_size = len(word2idx)\n",
    "        E = np.zeros((vocab_size, embedding_dim))\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.word_vec(word)\n",
    "            except KeyError:   # word not in embedding\n",
    "                pass\n",
    "            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n",
    "            #     pass\n",
    "        np.save(embedding_file, E)\n",
    "    return E\n",
    "\n",
    "\n",
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_sz, embed_sz, input_length,\n",
    "            num_filters, kernel_sz, output_sz, \n",
    "            run_mode, embedding_weights, \n",
    "            **kwargs):\n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        if run_mode == \"scratch\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                trainable=True)\n",
    "        elif run_mode == \"vectorizer\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=False)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=True)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
    "            kernel_size=kernel_sz,\n",
    "            activation=\"relu\")\n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dense = tf.keras.layers.Dense(output_sz, \n",
    "            activation=\"softmax\"\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# data distribution is 4827 ham and 747 spam (total 5574), which \n",
    "# works out to approx 87% ham and 13% spam, so we take reciprocals\n",
    "# and this works out to being each spam (1) item as being approximately\n",
    "# 8 times as important as each ham (0) message.\n",
    "CLASS_WEIGHTS = { 0: 1, 1: 8 }\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--mode\", help=\"run mode\",\n",
    "    choices=[\n",
    "        \"scratch\",\n",
    "        \"vectorizer\",\n",
    "        \"finetuning\"\n",
    "    ])\n",
    "args = parser.parse_args()\n",
    "run_mode = args.mode\n",
    "\n",
    "# read data\n",
    "texts, labels = download_and_read(DATASET_URL)\n",
    "\n",
    "# tokenize and pad text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "num_records = len(text_sequences)\n",
    "max_seqlen = len(text_sequences[0])\n",
    "print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))\n",
    "\n",
    "# labels\n",
    "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "# vocabulary\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word[0] = \"PAD\"\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size: {:d}\".format(vocab_size))\n",
    "\n",
    "# dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = num_records // 4\n",
    "val_size = (num_records - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# embedding\n",
    "E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,\n",
    "    EMBEDDING_NUMPY_FILE)\n",
    "print(\"Embedding matrix:\", E.shape)\n",
    "\n",
    "# model definition\n",
    "conv_num_filters = 256\n",
    "conv_kernel_size = 3\n",
    "model = SpamClassifierModel(\n",
    "    vocab_size, EMBEDDING_DIM, max_seqlen, \n",
    "    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
    "    run_mode, E)\n",
    "model.build(input_shape=(None, max_seqlen))\n",
    "model.summary()\n",
    "\n",
    "# compile and train\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# train model\n",
    "model.fit(train_dataset, epochs=NUM_EPOCHS, \n",
    "    validation_data=val_dataset,\n",
    "    class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "# evaluate against test set\n",
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest.tolist())\n",
    "\n",
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
