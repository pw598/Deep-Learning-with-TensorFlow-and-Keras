{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ecbfe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.gutenberg.org/cache/epub/28885/pg28885.txt\n",
      "180224/177441 [==============================] - 0s 0us/step\n",
      "188416/177441 [===============================] - 0s 0us/step\n",
      "Downloading data from https://www.gutenberg.org/files/12/12-0.txt\n",
      "196608/196464 [==============================] - 0s 0us/step\n",
      "204800/196464 [===============================] - 0s 0us/step\n",
      "vocab size: 93\n",
      "input:[The Project Gutenberg eBook of Alice's Adventures in Wonderland This ebook is for the use of anyone ]\n",
      "output:[he Project Gutenberg eBook of Alice's Adventures in Wonderland This ebook is for the use of anyone a]\n",
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n",
      "Model: \"char_gen_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  23808     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  107400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  9393      \n",
      "=================================================================\n",
      "Total params: 140,601\n",
      "Trainable params: 140,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(64, 100, 93)\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 3s 43ms/step - loss: 3.5013\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 2.7824\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 2.4728\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 2.3212\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 2.2147\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 2.1246\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 2.0502\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 1.9867\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 1.9280\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.8854\n",
      "after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1\n",
      "Alice the \"Afroume hey larthem une. and wongs it’ly noh me damring bus King fily in rom, yourd angost, wind my as_mongabmer att hind a curs if the Prempry Proj0czecer pare whe mugh ingray of combentir. “The to con works this thing a revent dake not this or she fartede.” “The Howerpioning she matt. “Swat youlf'tly, and like hell to opcrid was aroulitulfine, Wabled ho casese nop I_ she lork lica soptrive. ”’d car the _int woy of mad the Litcig, bronys ehined ievet exved tid.\" the has entain--wonsterg strenm, aster. Alice _arminglw duntion weldly dook to it you knoply Aside, bus So repwomster sumest,\" shattedoone?” “Yaring; shere nouted: hIt take all insttem dnak erman: comup me mandeded to brouge Dolly she) briction. \"Itly. \"Low the gade gon’t a co shat to k ofU out mare whe sattlying lock the dane have I gect wame round amry mid. \"You le the Tur be it tians—ereathing, abouk a concan to got alday tead wert quite a dorpedent. \"Xit the bat wfupponear’s with, and ruc_ exFurpered terssele rogherve\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 3s 43ms/step - loss: 1.8415\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.8048\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 1.7728\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.7429\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 1.7188\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.6978\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.6746\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.6513\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 1.6365\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.6158\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2\n",
      "Alice and thind some; Ly in a ragent— one of round, beffe!” the Queened hap again that All [Iite reprised Alice said Alice, thought_ their is this havet added winioned his dos into whission. Whill Looking. “I ne$p 1..]S. eh, as said looks alsto went or soke the Ducking dive, and she she hadd? Stath of the Projectire that having that's missed?” Alice: “to make a Caterphere the Queene: “I larte Colaironising only fissen gurt Alice an witery-poss the hourmant, a noigh dished To Alice darame Queen. \"Well glail on, when, as she, but as Alice, \"nely, as very them the caling to Time. Look a reas, with—and were noting, and a little plied off every have now when you with way to a puring so lesad_ and). Protever and where all the when what breathed it worderse she othacubl, you know through vownttrover in vett's a compedh of higgiths Project Guten, tread, no your fort, you diff firsted hear of the some of appew the plepale tan’, has and to spied whyer the tarmis. It was like in the elove of goush!” “i\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 3s 43ms/step - loss: 1.6029\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.5910\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 1.5736\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 1.5656\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.5524\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.5398\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.5339\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.5176\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 1.5084\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 1.5050\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3\n",
      "Alice ashes upsant of me,\" the resumpt she very here with up cish was hur wore, and was to know the jurtice fratentuned never began for for three to Alice, she stake _improridy was gaulde or what with exclanclegain whisuerd! Found?\" Alice aske_’t Licels ruse--you dear off non of the Of hell got that with her tet the Doire Queen found at it my real--that compluders—whrup of gratily! fot the eBook. Sexter pat hands or then't must have nast,” ha could rabotfully enches after your wouldn’t one with, by a has ratuerly voice; “and had things. To un two child round— And that it was_ with-and thinking to medents you get talk can Take the terthing a rathing There was the sawaticwos!” said the thing you whead!” asherg-wexty, a fafter usenbled gurpitily. Well, not surpray-- works, bronsled the quest in as her sweesces-lion, it’s a land--and Alice with mattle. The Queen: “Why, the White Queen all, and, To they of them, to one-times cunarouted. “What happen!\" shick Duchess, ave don’t's same sortrose much\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 3s 44ms/step - loss: 1.4949\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.4858\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 1.4821\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 1.4703\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 1.4656\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.4587\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 1.4557\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 1.4505\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.4409\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.4395\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4\n",
      "Alice asked!\" Carple, for the comp)—!\" the Macuts Toot dreasures, I shoudder which gone deEnd he found all was pretended a look off.'\" \"I was her herper. De-to me wentred withing, as when you’ll brown!” this even began palt slose till he ferum.\" As the Red Quee madion. BI live. \"I migBing coust-buging speam alom will melings.' “The White Kicle, asmarted as will. “I did--\" \"Tow a little was thing and thinking I cateroncing all as all to mery sighty under up _assturect stood dealsed (you can!\" saw she was herself, “The fellows with ort turing 1.3.!” the Shourster: “Fould, For an-Youmpy disonly verplat all we took narmisucinish anxiousem, cone shise work and if with a mouse as inse Queen strarse,” like I s/'to repirstod to seep about, as a lobst. “ICare, and eamed anfor offess that it lardial To-dooking “Project Gutenberg; in sare used to some ropenfor partion up or that it Archive the Duchess, but out them lottle shill had breaf_ made any Werid,\" she said, of the Qeeen upry and fewn, naire, an\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 3s 44ms/step - loss: 1.4351\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 1.4268\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.4243\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.4178\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 1.4169\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.4109\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 1.4091\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 1.4051\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 1.4019\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 1.3997\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5\n",
      "Alice to Hatten better to manatonsture it into accept so we’ve a little exclace in the couce.’ \"You were becan so reaurel to say think if I altaat is, to you’ll sclumbling of like beepp with a little time (when it a----\" \"Whoulfurwaby: \"chibse. If it's among the March smetendly, when you can very pleased, I did obtally: “_---and_ anything with off this very act alar hurrisibly--forch Hare, because she suire tay’s great twough,’ I should striglia, Alice country unecuboted time!” could have a liaging hishing 'ne concuring at the vounchly to make in at alling! Cot as he peated upon, as soined are longed the joprone what it do do rather whitch side_,” Adite flain.\" \"I the Queen seemed. And a tone, to her Take a shell not notled sogs hall she people look ear began'bute little child le: that's my! He fein marked in a remark ever to you. \"That is shoes. Which there was good at teamed in a misten to lean!\" said the Mock Trabmes of this: The crecent of trademing to get the kidiretshe feet of the glad\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "CHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")\n",
    "LOG_DIR = os.path.join(DATA_DIR, \"logs\")\n",
    "\n",
    "\n",
    "def clean_logs():\n",
    "    shutil.rmtree(CHECKPOINT_DIR, ignore_errors=True)\n",
    "    shutil.rmtree(LOG_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "def download_and_read(urls):\n",
    "    texts = []\n",
    "    for i, url in enumerate(urls):\n",
    "        p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url,\n",
    "            cache_dir=\".\")\n",
    "        text = open(p, mode=\"r\", encoding=\"utf-8\").read()\n",
    "        # remove byte order mark\n",
    "        text = text.replace(\"\\ufeff\", \"\")\n",
    "        # remove newlines\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(r'\\s+', \" \", text)\n",
    "        # add it to the list\n",
    "        texts.extend(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def split_train_labels(sequence):\n",
    "    input_seq = sequence[0:-1]\n",
    "    output_seq = sequence[1:]\n",
    "    return input_seq, output_seq\n",
    "\n",
    "\n",
    "class CharGenModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, num_timesteps, \n",
    "            embedding_dim, **kwargs):\n",
    "        super(CharGenModel, self).__init__(**kwargs)\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim\n",
    "        )\n",
    "        self.rnn_layer = tf.keras.layers.GRU(\n",
    "            num_timesteps,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            stateful=True,\n",
    "            return_sequences=True\n",
    "        )\n",
    "        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.rnn_layer(x)\n",
    "        x = self.dense_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def loss(labels, predictions):\n",
    "    return tf.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        predictions,\n",
    "        from_logits=True\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_text(model, prefix_string, char2idx, idx2char,\n",
    "        num_chars_to_generate=1000, temperature=1.0):\n",
    "    input = [char2idx[s] for s in prefix_string]\n",
    "    input = tf.expand_dims(input, 0)\n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    for i in range(num_chars_to_generate):\n",
    "        preds = model(input)\n",
    "        preds = tf.squeeze(preds, 0) / temperature\n",
    "        # predict char returned by model\n",
    "        pred_id = tf.random.categorical(preds, num_samples=1)[-1, 0].numpy()\n",
    "        text_generated.append(idx2char[pred_id])\n",
    "        # pass the prediction as the next input to the model\n",
    "        input = tf.expand_dims([pred_id], 0)\n",
    "\n",
    "    return prefix_string + \"\".join(text_generated)\n",
    "\n",
    "\n",
    "# download and read into local data structure (list of chars)\n",
    "texts = download_and_read([\n",
    "    \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n",
    "    \"https://www.gutenberg.org/files/12/12-0.txt\"\n",
    "])\n",
    "clean_logs()\n",
    "\n",
    "# create the vocabulary\n",
    "vocab = sorted(set(texts))\n",
    "print(\"vocab size: {:d}\".format(len(vocab)))\n",
    "\n",
    "# create mapping from vocab chars to ints\n",
    "char2idx = {c:i for i, c in enumerate(vocab)}\n",
    "idx2char = {i:c for c, i in char2idx.items()}\n",
    "\n",
    "# numericize the texts\n",
    "texts_as_ints = np.array([char2idx[c] for c in texts])\n",
    "data = tf.data.Dataset.from_tensor_slices(texts_as_ints)\n",
    "\n",
    "# number of characters to show before asking for prediction\n",
    "# sequences: [None, 100]\n",
    "seq_length = 100\n",
    "sequences = data.batch(seq_length + 1, drop_remainder=True)\n",
    "sequences = sequences.map(split_train_labels)\n",
    "\n",
    "# print out input and output to see what they look like\n",
    "for input_seq, output_seq in sequences.take(1):\n",
    "    print(\"input:[{:s}]\".format(\n",
    "        \"\".join([idx2char[i] for i in input_seq.numpy()])))\n",
    "    print(\"output:[{:s}]\".format(\n",
    "        \"\".join([idx2char[i] for i in output_seq.numpy()])))\n",
    "\n",
    "# set up for training\n",
    "# batches: [None, 64, 100]\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(texts) // seq_length // batch_size\n",
    "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "print(dataset)\n",
    "\n",
    "# define network\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "\n",
    "model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
    "model.build(input_shape=(batch_size, seq_length))\n",
    "model.summary()\n",
    "\n",
    "# try running some data through the model to validate dimensions\n",
    "for input_batch, label_batch in dataset.take(1):\n",
    "    pred_batch = model(input_batch)\n",
    "\n",
    "print(pred_batch.shape)\n",
    "assert(pred_batch.shape[0] == batch_size)\n",
    "assert(pred_batch.shape[1] == seq_length)\n",
    "assert(pred_batch.shape[2] == vocab_size)\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)\n",
    "\n",
    "# we will train our model for 50 epochs, and after every 10 epochs\n",
    "# we want to see how well it will generate text\n",
    "num_epochs = 50\n",
    "for i in range(num_epochs // 10):\n",
    "    model.fit(\n",
    "        dataset.repeat(),\n",
    "        epochs=10,\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "        # callbacks=[checkpoint_callback, tensorboard_callback]\n",
    "    )\n",
    "    checkpoint_file = os.path.join(\n",
    "        CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n",
    "    model.save_weights(checkpoint_file)\n",
    "\n",
    "    # create a generative model using the trained model so far\n",
    "    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
    "    gen_model.load_weights(checkpoint_file)\n",
    "    gen_model.build(input_shape=(1, seq_length))\n",
    "\n",
    "    print(\"after epoch: {:d}\".format(i+1)*10)\n",
    "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n",
    "    print(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
